{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12834346,"sourceType":"datasetVersion","datasetId":8116964}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n\n\n!pip install lime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:00:48.902668Z","iopub.execute_input":"2025-08-22T08:00:48.902843Z","iopub.status.idle":"2025-08-22T08:02:12.214689Z","shell.execute_reply.started":"2025-08-22T08:00:48.902825Z","shell.execute_reply":"2025-08-22T08:02:12.213929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom torchvision.transforms import ToTensor\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom pytorch_grad_cam import GradCAM, GradCAMPlusPlus, EigenCAM, AblationCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom lime import lime_image\nimport zipfile\nimport os\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:02:48.275897Z","iopub.execute_input":"2025-08-22T08:02:48.276179Z","iopub.status.idle":"2025-08-22T08:02:55.872941Z","shell.execute_reply.started":"2025-08-22T08:02:48.276148Z","shell.execute_reply":"2025-08-22T08:02:55.872343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and Prepare the Dataset","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ndataset = datasets.ImageFolder(root=\"/kaggle/input/bangladeshi-mango-leaf3/Image Dataset of Bangladeshi Mango Leaf/Root/Root/Original\", transform=transform_train)\nclass_names = dataset.classes\nnum_classes = len(class_names)\n\nprint(\"Classes found:\", class_names)\nprint(\"Number of classes:\", num_classes)\n\n\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.2 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\nval_dataset.dataset.transform = transform_test\ntest_dataset.dataset.transform = transform_test\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:03:43.664849Z","iopub.execute_input":"2025-08-22T08:03:43.665583Z","iopub.status.idle":"2025-08-22T08:03:44.660873Z","shell.execute_reply.started":"2025-08-22T08:03:43.665558Z","shell.execute_reply":"2025-08-22T08:03:44.660280Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize Example Images for Each Class","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, num_classes, figsize=(15, 5))\ndisplayed_classes = {class_name: False for class_name in class_names}\n\nfor images, labels in train_loader:\n    for img, label in zip(images, labels):\n        class_name = class_names[label]\n        if not displayed_classes[class_name]:\n            img = img.permute(1, 2, 0).numpy()\n            img = (img * 0.5) + 0.5  # unnormalize\n            axs[label].imshow(np.clip(img, 0, 1))\n            axs[label].set_title(class_name)\n            axs[label].axis('off')\n            displayed_classes[class_name] = True\n\n        if all(displayed_classes.values()):\n            break\n    if all(displayed_classes.values()):\n        break\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:04:10.822184Z","iopub.execute_input":"2025-08-22T08:04:10.822869Z","iopub.status.idle":"2025-08-22T08:04:11.548767Z","shell.execute_reply.started":"2025-08-22T08:04:10.822847Z","shell.execute_reply":"2025-08-22T08:04:11.547933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer Learning Models","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\n\ndef get_transfer_model(model_name, num_classes):\n    if model_name == \"vgg16\":\n        model = models.vgg16(pretrained=True)\n        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n\n    elif model_name == \"mobilenet_v2\":\n        model = models.mobilenet_v2(pretrained=True)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n\n    elif model_name == \"efficientnet_b0\":\n        model = models.efficientnet_b0(pretrained=True)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n\n    elif model_name == \"densenet121\":\n        model = models.densenet121(pretrained=True)\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n\n    elif model_name == \"inception_v3\":\n        model = models.inception_v3(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n\n    elif model_name == \"resnet50\":\n        model = models.resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)  \n\n    else:\n        raise ValueError(f\"Model '{model_name}' not supported. Choose from vgg16, mobilenet_v2, efficientnet_b0, densenet121, inception_v3, resnet50.\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:06:02.214203Z","iopub.execute_input":"2025-08-22T08:06:02.214510Z","iopub.status.idle":"2025-08-22T08:06:02.221013Z","shell.execute_reply.started":"2025-08-22T08:06:02.214456Z","shell.execute_reply":"2025-08-22T08:06:02.220164Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Early Stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n\n    def __init__(self, patience=5):\n\n        self.patience = patience\n        self.counter = 0\n        self.best_loss = np.inf\n\n    def check_early_stop(self, val_loss):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:06:32.398166Z","iopub.execute_input":"2025-08-22T08:06:32.398428Z","iopub.status.idle":"2025-08-22T08:06:32.403249Z","shell.execute_reply.started":"2025-08-22T08:06:32.398410Z","shell.execute_reply":"2025-08-22T08:06:32.402517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer Learning Example using DenseNet121","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\n\nnum_epochs = 50\n\nmodel = get_transfer_model('densenet121', num_classes).to('cuda')\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nearly_stopping = EarlyStopping(patience=5)\n\ntrain_losses, val_losses = [], []\n\nscaler = GradScaler()\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n    model.train()\n    train_loss = 0\n    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n        images, labels = images.to('cuda'), labels.to('cuda')\n        optimizer.zero_grad()\n\n        with autocast():\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item()\n\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n            images, labels = images.to('cuda'), labels.to('cuda')\n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n    avg_val_loss = val_loss / len(val_loader)\n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n\n    print(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n\n    if early_stopping.check_early_stop(avg_val_loss):\n        print(\"Early stopping triggered.\")\n        break\n\n# Save model\ntorch.save(model.state_dict(), 'transfer_learning_densenet121.pth')\nprint(\"Model saved as 'transfer_learning_densenet121.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:07:29.868374Z","iopub.execute_input":"2025-08-22T08:07:29.868990Z","iopub.status.idle":"2025-08-22T08:08:43.402642Z","shell.execute_reply.started":"2025-08-22T08:07:29.868965Z","shell.execute_reply":"2025-08-22T08:08:43.401896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting loss curves\n\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title(\"Training and Validation Loss for DenseNet-121\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:09:05.662358Z","iopub.execute_input":"2025-08-22T08:09:05.663081Z","iopub.status.idle":"2025-08-22T08:09:05.825549Z","shell.execute_reply.started":"2025-08-22T08:09:05.663055Z","shell.execute_reply":"2025-08-22T08:09:05.824845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation and Metrics Calculation","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nmodel.eval()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():  \n    for images, labels in test_loader:\n        \n        images, labels = images.to('cuda'), labels.to('cuda')\n\n        outputs = model(images)\n\n        _, predicted = torch.max(outputs, 1)\n\n        all_preds.extend(predicted.cpu().numpy())  \n        all_labels.extend(labels.cpu().numpy())    \n\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:09:54.397101Z","iopub.execute_input":"2025-08-22T08:09:54.397424Z","iopub.status.idle":"2025-08-22T08:09:55.389150Z","shell.execute_reply.started":"2025-08-22T08:09:54.397404Z","shell.execute_reply":"2025-08-22T08:09:55.388530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XAI - Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM","metadata":{}},{"cell_type":"code","source":"from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n\ndef get_transfer_model(model_name, num_classes):\n    if model_name == \"densenet121\":\n        model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n    else:\n        raise ValueError(f\"Model '{model_name}' not supported.\")\n    return model\n\nmodel = get_transfer_model('densenet121', num_classes).to('cuda')\nmodel.load_state_dict(torch.load('transfer_learning_densenet121.pth'))  \nmodel.eval()\n\nsample_idx = 0  \nsample_image, true_label = test_dataset[sample_idx]\nsample_image = sample_image.unsqueeze(0).to('cuda')  \n\noriginal_image_np = sample_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\noriginal_image_np = (original_image_np * 0.5) + 0.5  \noriginal_image_np = np.clip(original_image_np, 0, 1)\n\ntarget_layers = [model.features.norm5]  \n\ngradcam = GradCAM(model=model, target_layers=target_layers)\ngradcam_plus_plus = GradCAMPlusPlus(model=model, target_layers=target_layers)\neigen_cam = EigenCAM(model=model, target_layers=target_layers)\nablation_cam = AblationCAM(model=model, target_layers=target_layers)\n\nwith torch.no_grad():\n    outputs = model(sample_image)\n    predicted_class = outputs.argmax().item()\n    predicted_class_name = class_names[predicted_class]\n    true_class_name = class_names[true_label]\n\ntarget = [ClassifierOutputTarget(predicted_class)]\n\ngradcam_heatmap = gradcam(input_tensor=sample_image, targets=target)[0]\ngradcam_pp_heatmap = gradcam_plus_plus(input_tensor=sample_image, targets=target)[0]\neigen_cam_heatmap = eigen_cam(input_tensor=sample_image, targets=target)[0]\nablation_cam_heatmap = ablation_cam(input_tensor=sample_image, targets=target)[0]\n\ngradcam_result = show_cam_on_image(original_image_np, gradcam_heatmap, use_rgb=True)\ngradcam_pp_result = show_cam_on_image(original_image_np, gradcam_pp_heatmap, use_rgb=True)\neigen_cam_result = show_cam_on_image(original_image_np, eigen_cam_heatmap, use_rgb=True)\nablation_cam_result = show_cam_on_image(original_image_np, ablation_cam_heatmap, use_rgb=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:38:20.509970Z","iopub.execute_input":"2025-08-22T08:38:20.510158Z","iopub.status.idle":"2025-08-22T08:38:24.189898Z","shell.execute_reply.started":"2025-08-22T08:38:20.510144Z","shell.execute_reply":"2025-08-22T08:38:24.189158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the results\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 5, 1)\nplt.imshow(original_image_np)\nplt.title(f\"Original Image\\n(True: {true_class_name}, Pred: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 2)\nplt.imshow(gradcam_result)\nplt.title(f\"Grad-CAM\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 3)\nplt.imshow(gradcam_pp_result)\nplt.title(f\"Grad-CAM++\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 4)\nplt.imshow(eigen_cam_result)\nplt.title(f\"Eigen-CAM\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 5)\nplt.imshow(ablation_cam_result)\nplt.title(f\"Ablation-CAM\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:38:29.297091Z","iopub.execute_input":"2025-08-22T08:38:29.297367Z","iopub.status.idle":"2025-08-22T08:38:29.908206Z","shell.execute_reply.started":"2025-08-22T08:38:29.297346Z","shell.execute_reply":"2025-08-22T08:38:29.907395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LIME","metadata":{}},{"cell_type":"code","source":"from lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\ndef batch_predict(images):\n    model.eval()\n    \n    batch = torch.stack([\n        transform_test(Image.fromarray((image * 255).astype(np.uint8))) \n        for image in images\n    ], dim=0).to('cuda')\n    \n    with torch.no_grad():\n        logits = model(batch)\n    \n    return torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n\nexplainer = lime_image.LimeImageExplainer()\n\nlime_explanation = explainer.explain_instance(\n    original_image_np,\n    batch_predict,\n    top_labels=1,       \n    hide_color=0,\n    num_samples=100    \n)\n\nlime_image, lime_mask = lime_explanation.get_image_and_mask(\n    label=predicted_class,\n    positive_only=True,\n    hide_rest=False,\n    num_features=10,\n    min_weight=0.01\n)\nlime_image = mark_boundaries(lime_image, lime_mask)\n\n# Display the original and LIME result\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(original_image_np)\nplt.title(f\"Original Image\\n(True: {true_class_name}, Pred: {predicted_class_name})\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(lime_image)\nplt.title(f\"LIME Explanation\\n(Predicted: {predicted_class_name})\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:38:40.037414Z","iopub.execute_input":"2025-08-22T08:38:40.038129Z","iopub.status.idle":"2025-08-22T08:38:41.681280Z","shell.execute_reply.started":"2025-08-22T08:38:40.038103Z","shell.execute_reply":"2025-08-22T08:38:41.680504Z"}},"outputs":[],"execution_count":null}]}