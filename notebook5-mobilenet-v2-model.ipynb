{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12835426,"sourceType":"datasetVersion","datasetId":8117704}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n\n\n!pip install lime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:14:16.028762Z","iopub.execute_input":"2025-08-22T10:14:16.029417Z","iopub.status.idle":"2025-08-22T10:15:52.921534Z","shell.execute_reply.started":"2025-08-22T10:14:16.029393Z","shell.execute_reply":"2025-08-22T10:15:52.920577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom torchvision.transforms import ToTensor\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom pytorch_grad_cam import GradCAM, GradCAMPlusPlus, EigenCAM, AblationCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom lime import lime_image\nimport zipfile\nimport os\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:16:26.381038Z","iopub.execute_input":"2025-08-22T10:16:26.381606Z","iopub.status.idle":"2025-08-22T10:16:37.822190Z","shell.execute_reply.started":"2025-08-22T10:16:26.381563Z","shell.execute_reply":"2025-08-22T10:16:37.821429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and Prepare the Dataset","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ndataset = datasets.ImageFolder(root=\"/kaggle/input/bangladeshi-mango-leaf5/Image Dataset of Bangladeshi Mango Leaf/Root/Root/Original\", transform=transform_train)\nclass_names = dataset.classes\nnum_classes = len(class_names)\n\nprint(\"Classes found:\", class_names)\nprint(\"Number of classes:\", num_classes)\n\n\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.2 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\nval_dataset.dataset.transform = transform_test\ntest_dataset.dataset.transform = transform_test\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:17:45.576591Z","iopub.execute_input":"2025-08-22T10:17:45.577307Z","iopub.status.idle":"2025-08-22T10:17:47.268317Z","shell.execute_reply.started":"2025-08-22T10:17:45.577281Z","shell.execute_reply":"2025-08-22T10:17:47.267565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize Example Images for Each Class","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, num_classes, figsize=(15, 5))\ndisplayed_classes = {class_name: False for class_name in class_names}\n\nfor images, labels in train_loader:\n    for img, label in zip(images, labels):\n        class_name = class_names[label]\n        if not displayed_classes[class_name]:\n            img = img.permute(1, 2, 0).numpy()\n            img = (img * 0.5) + 0.5  # unnormalize\n            axs[label].imshow(np.clip(img, 0, 1))\n            axs[label].set_title(class_name)\n            axs[label].axis('off')\n            displayed_classes[class_name] = True\n\n        if all(displayed_classes.values()):\n            break\n    if all(displayed_classes.values()):\n        break\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:18:16.883749Z","iopub.execute_input":"2025-08-22T10:18:16.884498Z","iopub.status.idle":"2025-08-22T10:18:17.737845Z","shell.execute_reply.started":"2025-08-22T10:18:16.884464Z","shell.execute_reply":"2025-08-22T10:18:17.736956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer Learning Models","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\n\ndef get_transfer_model(model_name, num_classes):\n    if model_name == \"vgg16\":\n        model = models.vgg16(pretrained=True)\n        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n\n    elif model_name == \"mobilenet_v2\":\n        model = models.mobilenet_v2(pretrained=True)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n\n    elif model_name == \"efficientnet_b0\":\n        model = models.efficientnet_b0(pretrained=True)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n\n    elif model_name == \"densenet121\":\n        model = models.densenet121(pretrained=True)\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n\n    elif model_name == \"inception_v3\":\n        model = models.inception_v3(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n\n    elif model_name == \"resnet50\":\n        model = models.resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)  \n\n    else:\n        raise ValueError(f\"Model '{model_name}' not supported. Choose from vgg16, mobilenet_v2, efficientnet_b0, densenet121, inception_v3, resnet50.\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:19:17.930741Z","iopub.execute_input":"2025-08-22T10:19:17.931070Z","iopub.status.idle":"2025-08-22T10:19:17.937910Z","shell.execute_reply.started":"2025-08-22T10:19:17.931037Z","shell.execute_reply":"2025-08-22T10:19:17.937361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Early Stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n\n    def __init__(self, patience=5):\n\n        self.patience = patience\n        self.counter = 0\n        self.best_loss = np.inf\n\n    def check_early_stop(self, val_loss):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:19:44.564128Z","iopub.execute_input":"2025-08-22T10:19:44.564402Z","iopub.status.idle":"2025-08-22T10:19:44.569249Z","shell.execute_reply.started":"2025-08-22T10:19:44.564380Z","shell.execute_reply":"2025-08-22T10:19:44.568555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer Learning Example using MobileNet-V2","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\n\nnum_epochs = 50\n\nmodel = get_transfer_model('mobilenet_v2', num_classes).to('cuda')\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nearly_stopping = EarlyStopping(patience=5)\n\ntrain_losses, val_losses = [], []\nscaler = GradScaler()\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n    model.train()\n    train_loss = 0\n    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n        images, labels = images.to('cuda'), labels.to('cuda')\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        train_loss += loss.item()\n\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n            images, labels = images.to('cuda'), labels.to('cuda')\n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n    avg_val_loss = val_loss / len(val_loader)\n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n    print(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n\n    if early_stopping.check_early_stop(avg_val_loss):\n        print(\"Early stopping triggered.\")\n        break\n\ntorch.save(model.state_dict(), 'transfer_learning_mobilenetv2.pth')\nprint(\"Model saved as 'transfer_learning_mobilenetv2.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:35:17.890043Z","iopub.execute_input":"2025-08-22T10:35:17.890818Z","iopub.status.idle":"2025-08-22T10:36:11.435200Z","shell.execute_reply.started":"2025-08-22T10:35:17.890774Z","shell.execute_reply":"2025-08-22T10:36:11.434576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting loss curves\n\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title(\"Training and Validation Loss for MobileNet-V2\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:36:28.363753Z","iopub.execute_input":"2025-08-22T10:36:28.364399Z","iopub.status.idle":"2025-08-22T10:36:28.521857Z","shell.execute_reply.started":"2025-08-22T10:36:28.364374Z","shell.execute_reply":"2025-08-22T10:36:28.521030Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation and Metrics Calculation","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nmodel.eval()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():  \n    for images, labels in test_loader:\n        \n        images, labels = images.to('cuda'), labels.to('cuda')\n\n        outputs = model(images)\n\n        _, predicted = torch.max(outputs, 1)\n\n        all_preds.extend(predicted.cpu().numpy())  \n        all_labels.extend(labels.cpu().numpy())    \n\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:36:35.014498Z","iopub.execute_input":"2025-08-22T10:36:35.015115Z","iopub.status.idle":"2025-08-22T10:36:35.681340Z","shell.execute_reply.started":"2025-08-22T10:36:35.015090Z","shell.execute_reply":"2025-08-22T10:36:35.680543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XAI - Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM","metadata":{}},{"cell_type":"code","source":"from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nimport cv2\n\ndef get_transfer_model(model_name, num_classes):\n    if model_name == \"mobilenet_v2\":\n        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    else:\n        raise ValueError(f\"Model '{model_name}' not supported.\")\n    return model\n\nmodel = get_transfer_model('mobilenet_v2', num_classes).to('cuda')\nmodel.load_state_dict(torch.load('transfer_learning_mobilenetv2.pth'))  \nmodel.eval()\n\nsample_idx = 0  \nsample_image, true_label = test_dataset[sample_idx]\nsample_image = sample_image.unsqueeze(0).to('cuda')  \n\noriginal_image_np = sample_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\noriginal_image_np = (original_image_np * 0.5) + 0.5  \noriginal_image_np = np.clip(original_image_np, 0, 1)\n\ntarget_layers = [model.features[-1]]  \n\ngradcam = GradCAM(model=model, target_layers=target_layers)\ngradcam_plus_plus = GradCAMPlusPlus(model=model, target_layers=target_layers)\neigen_cam = EigenCAM(model=model, target_layers=target_layers)\nablation_cam = AblationCAM(model=model, target_layers=target_layers)\n\nwith torch.no_grad():\n    outputs = model(sample_image)\n    predicted_class = outputs.argmax().item()\n    predicted_class_name = class_names[predicted_class]\n    true_class_name = class_names[true_label]\n\ntarget = [ClassifierOutputTarget(predicted_class)]\n\ngradcam_heatmap = gradcam(input_tensor=sample_image, targets=target)[0]\ngradcam_pp_heatmap = gradcam_plus_plus(input_tensor=sample_image, targets=target)[0]\neigen_cam_heatmap = eigen_cam(input_tensor=sample_image, targets=target)[0]\nablation_cam_heatmap = ablation_cam(input_tensor=sample_image, targets=target)[0]\n\ndef enhanced_show_cam_on_image(img, mask, use_rgb=True, colormap=cv2.COLORMAP_JET, image_weight=0.5):\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n    if use_rgb:\n        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    heatmap = np.float32(heatmap) / 255\n    cam = heatmap + np.float32(img)\n    cam = cam / np.max(cam)\n    return np.uint8(255 * cam)\n\ngradcam_result = enhanced_show_cam_on_image(original_image_np, gradcam_heatmap, image_weight=0.6)\ngradcam_pp_result = enhanced_show_cam_on_image(original_image_np, gradcam_pp_heatmap, image_weight=0.6)\neigen_cam_result = enhanced_show_cam_on_image(original_image_np, eigen_cam_heatmap, image_weight=0.6)\nablation_cam_result = enhanced_show_cam_on_image(original_image_np, ablation_cam_heatmap, image_weight=0.6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:48:29.614850Z","iopub.execute_input":"2025-08-22T10:48:29.615434Z","iopub.status.idle":"2025-08-22T10:48:31.355977Z","shell.execute_reply.started":"2025-08-22T10:48:29.615412Z","shell.execute_reply":"2025-08-22T10:48:31.355424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the results\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 5, 1)\nplt.imshow(original_image_np)\nplt.title(f\"Original Image\\n(True: {true_class_name}, Pred: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 2)\nplt.imshow(gradcam_result)\nplt.title(f\"Grad-CAM\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 3)\nplt.imshow(gradcam_pp_result)\nplt.title(f\"Grad-CAM++\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 4)\nplt.imshow(eigen_cam_result)\nplt.title(f\"Eigen-CAM\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.subplot(1, 5, 5)\nplt.imshow(ablation_cam_result)\nplt.title(f\"Ablation-CAM\\n(Predicted: {predicted_class_name})\", fontsize=10)\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:49:06.188672Z","iopub.execute_input":"2025-08-22T10:49:06.188922Z","iopub.status.idle":"2025-08-22T10:49:06.783175Z","shell.execute_reply.started":"2025-08-22T10:49:06.188906Z","shell.execute_reply":"2025-08-22T10:49:06.782254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LIME","metadata":{}},{"cell_type":"code","source":"from lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\ndef batch_predict(images):\n    model.eval()\n    \n    batch = torch.stack([\n        transform_test(Image.fromarray((image * 255).astype(np.uint8))) \n        for image in images\n    ], dim=0).to('cuda')\n    \n    with torch.no_grad():\n        logits = model(batch)\n    \n    return torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n\nexplainer = lime_image.LimeImageExplainer()\n\nlime_explanation = explainer.explain_instance(\n    original_image_np,\n    batch_predict,\n    top_labels=1,\n    hide_color=0,\n    num_samples=100    \n)\n\nlime_image, lime_mask = lime_explanation.get_image_and_mask(\n    label=predicted_class,\n    positive_only=True,\n    hide_rest=False,\n    num_features=10,\n    min_weight=0.01\n)\nlime_image = mark_boundaries(lime_image, lime_mask)\n\n# Display the original and LIME result\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(original_image_np)\nplt.title(f\"Original Image\\n(True: {true_class_name}, Pred: {predicted_class_name})\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(lime_image)\nplt.title(f\"LIME Explanation\\n(Predicted: {predicted_class_name})\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:51:24.546998Z","iopub.execute_input":"2025-08-22T10:51:24.547278Z","iopub.status.idle":"2025-08-22T10:51:26.022448Z","shell.execute_reply.started":"2025-08-22T10:51:24.547258Z","shell.execute_reply":"2025-08-22T10:51:26.021647Z"}},"outputs":[],"execution_count":null}]}